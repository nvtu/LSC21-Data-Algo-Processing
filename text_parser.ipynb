{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from textblob import Word, TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import contractions\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import json\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def extract_noun_phrases(pos_tags):\n",
    "\n",
    "    def parse_noun_phrase(indices, num_tokens):\n",
    "        \"\"\"\n",
    "        Input of this function is the indices of possible begining positions of a noun phrases and the number of tokens\n",
    "        \"\"\"\n",
    "        noun_phrases = []\n",
    "        for index in indices:\n",
    "            noun_phrase_tokens = []\n",
    "            adj_phrase_tokens = []\n",
    "            has_noun = False\n",
    "            for i in range(index + 1, num_tokens):\n",
    "                token, tag = pos_tags[i]\n",
    "                if tag == 'DT' or tag == 'CD':      # If the current token is article or cardinal, then we don't count it as a part of noun phrase.\n",
    "                    if has_noun is True: break\n",
    "                    else: continue\n",
    "                elif tag in ['NN', 'NNS', 'NNP', 'NNPS']:   # Obviously\n",
    "                    noun_phrase_tokens.append(token)\n",
    "                    has_noun = True\n",
    "                elif tag == 'JJ':       # If the current token is adjective, it is the sign of the beginning of a noun phrases.\n",
    "                    if has_noun is True: break      # If some nouns appear before the adjective token, it is not correct noun phrase --> therefore, stop.\n",
    "                    adj_phrase_tokens.append(token)\n",
    "                elif tag == 'IN':     # If current token is preposition\n",
    "                    if has_noun is True and token == 'of': # Consider the \"of\" preposition in the noun phrase\n",
    "                        noun_phrase_tokens.append(token)\n",
    "                    else: break\n",
    "                elif tag == 'VBG':      # If current token is gerund, it might be a noun\n",
    "                    if i + 1 == num_tokens: continue\n",
    "                    next_token, next_tag = pos_tags[i+1]\n",
    "                    if i < 1 and next_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "                        noun_phrase_tokens.append(token)\n",
    "                        continue\n",
    "                    prev_token, prev_tag = pos_tags[i-1] \n",
    "                    if prev_tag == 'IN' and prev_token == 'of' or next_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "                        noun_phrase_tokens.append(token)\n",
    "                else: break\n",
    "            noun_phrase = ' '.join(noun_phrase_tokens)\n",
    "            adj_phrase = ' '.join(adj_phrase_tokens)\n",
    "            if len(adj_phrase_tokens) > 0 and len(noun_phrase) > 0:\n",
    "                noun_phrase = adj_phrase + ' ' + noun_phrase\n",
    "            if len(noun_phrase) > 0:\n",
    "                noun_phrases.append(noun_phrase)\n",
    "            else: continue \n",
    "        return noun_phrases\n",
    "    num_tokens = len(pos_tags)\n",
    "    noun_phrases = []\n",
    "\n",
    "    # Brute force to find noun phrases\n",
    "    indices = [i-1 for i, item in enumerate(pos_tags) if item[1] in ['NN', 'NNS', 'NNP', 'NNPS'] or item[1] in ['JJ', 'JJR', 'JJS'] or item[1] == 'VBG']\n",
    "    noun_phrases += parse_noun_phrase(indices, num_tokens)\n",
    "    # Tokenize all possible tokens whose tags are noun\n",
    "    single_nouns = [item[0] for item in pos_tags if item[1] in ['NN', 'NNS', 'NNP', 'NNPS'] and item[0] not in noun_phrases]\n",
    "    noun_phrases += single_nouns\n",
    "    return noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def analyse(parsed_tokens):\n",
    "    filtered_tokens = [token for token in parsed_tokens if token in time_dict or token in location_dict or token in vc_dict]    # Compare the noun phrases with dictionaries to find matches terms\n",
    "    filtered_tokens = sorted(filtered_tokens, key=lambda x: len(x), reverse=True)\n",
    "    token_counter = Counter(filtered_tokens)\n",
    "    minus_counter = {}\n",
    "    for token in token_counter.keys():\n",
    "        minus_counter[token] = 0;\n",
    "    tagged_tokens = []\n",
    "    for token, cnt in token_counter.items():\n",
    "        cnt += minus_counter[token]\n",
    "        if cnt == 0: continue\n",
    "        word_tokens = nltk.word_tokenize(token)\n",
    "        if len(word_tokens) > 1:\n",
    "            for wtoken in word_tokens: # Reduce the number of single nouns which are included in a matched noun noun_phrases\n",
    "                try:\n",
    "                    minus_counter[wtoken] -= 1\n",
    "                except: continue\n",
    "        if token in time_dict:\n",
    "            tagged_tokens.append((token, 'TIME', cnt))\n",
    "        if token in location_dict:\n",
    "            tagged_tokens.append((token, 'LOCATION', cnt))\n",
    "        if token in vc_dict:\n",
    "            tagged_tokens.append((token, 'CONCEPT', cnt))\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokens(noun_phrases):\n",
    "    processed_words = []\n",
    "    porter_stem = PorterStemmer()\n",
    "    for word in noun_phrases:\n",
    "        tokens = nltk.word_tokenize(word.lower())\n",
    "        refined_tokenize_list = [unicodedata.normalize('NFKD', token).encode('ascii', 'ignore').decode('utf-8', 'ignore') for token in tokens]\n",
    "        refined_tokenize_list = [word for word in refined_tokenize_list if word not in stop_words]\n",
    "        refined_tokenize_list = [porter_stem.stem(word) for word in refined_tokenize_list]\n",
    "        refined_tokenize_list = [word for word in refined_tokenize_list if len(word) > 1 and word.isalpha()]\n",
    "        complete_word = ' '.join(refined_tokenize_list)\n",
    "        processed_words.append(complete_word)\n",
    "    return processed_words"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
