{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from textblob import Word, TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import contractions\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import json\n",
    "import numpy as np\n",
    "import itertools\n",
    "from utils import time_this\n",
    "import inflect\n",
    "from sutime import SUTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get nltk necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nvtu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nvtu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "home_path = osp.expanduser('~')\n",
    "stopword_folder_path = osp.join(home_path, 'nltk_data', 'corpora', 'stopwords')\n",
    "punkt_folder_path = osp.join(home_path, 'nltk_data', 'corpora', 'punkt')\n",
    "averaged_perceptron_tagger_path = osp.join(home_path, 'nltk_data', 'averaged_perceptron_tagger')\n",
    "if not osp.exists(stopword_folder_path):\n",
    "    nltk.download('stopwords')\n",
    "if not osp.exists(punkt_folder_path):\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun_phrases(pos_tags):\n",
    "\n",
    "    def parse_noun_phrase(indices, num_tokens):\n",
    "        \"\"\"\n",
    "        Input of this function is the indices of possible begining positions of a noun phrases and the number of tokens\n",
    "        \"\"\"\n",
    "        noun_phrases = []\n",
    "        for index in indices:\n",
    "            noun_phrase_tokens = []\n",
    "            adj_phrase_tokens = []\n",
    "            has_noun = False\n",
    "            has_cardinal = False\n",
    "            for i in range(index + 1, num_tokens):\n",
    "                token, tag = pos_tags[i]\n",
    "                if tag == 'DT':      # If the current token is article, then we don't count it as a part of noun phrase.\n",
    "                    if has_noun is True: break\n",
    "                    else: continue\n",
    "                elif tag == 'CD':\n",
    "                    if has_cardinal is True: break # It is non-sense if two cardinal digits appear together\n",
    "                    adj_phrase_tokens.append(token) # If some cardinal digit appears before a noun, it is counted as an adjective\n",
    "                    has_cardinal = True\n",
    "                elif tag in ['NN', 'NNS', 'NNP', 'NNPS']:   # Obviously\n",
    "                    noun_phrase_tokens.append(token)\n",
    "                    has_noun = True\n",
    "                elif tag == 'JJ':       # If the current token is adjective, it is the sign of the beginning of a noun phrases.\n",
    "                    if has_noun is True: break      # If some nouns appear before the adjective token, it is not correct noun phrase --> therefore, stop.\n",
    "                    adj_phrase_tokens.append(token)\n",
    "                elif tag == 'IN':     # If current token is preposition\n",
    "                    if has_noun is True and token == 'of': # Consider the \"of\" preposition in the noun phrase\n",
    "                        noun_phrase_tokens.append(token)\n",
    "                    else: break\n",
    "                elif tag == 'VBG':      # If current token is gerund, it might be a noun\n",
    "                    if i + 1 == num_tokens: continue\n",
    "                    next_token, next_tag = pos_tags[i+1]\n",
    "                    if i < 1 and next_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "                        noun_phrase_tokens.append(token)\n",
    "                        continue\n",
    "                    prev_token, prev_tag = pos_tags[i-1] \n",
    "                    if prev_tag == 'IN' and prev_token == 'of' or next_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "                        noun_phrase_tokens.append(token)\n",
    "                else: break\n",
    "            noun_phrase = ' '.join(noun_phrase_tokens)\n",
    "            adj_phrase = ' '.join(adj_phrase_tokens)\n",
    "            if len(adj_phrase_tokens) > 0 and len(noun_phrase) > 0:\n",
    "                noun_phrase = adj_phrase + ' ' + noun_phrase\n",
    "            if len(noun_phrase) > 0:\n",
    "                noun_phrases.append(noun_phrase)\n",
    "            else: continue \n",
    "        return noun_phrases\n",
    "    num_tokens = len(pos_tags)\n",
    "    noun_phrases = []\n",
    "\n",
    "    # Brute force to find noun phrases\n",
    "    indices = [i-1 for i, item in enumerate(pos_tags) if item[1] in ['NN', 'NNS', 'NNP', 'NNPS'] or item[1] in ['JJ', 'JJR', 'JJS'] or item[1] == 'VBG']\n",
    "    noun_phrases += parse_noun_phrase(indices, num_tokens)\n",
    "    # Tokenize all possible tokens whose tags are noun\n",
    "    single_nouns = [item[0] for item in pos_tags if item[1] in ['NN', 'NNS', 'NNP', 'NNPS'] and item[0] not in noun_phrases]\n",
    "    noun_phrases += single_nouns\n",
    "    return noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(parsed_tokens):\n",
    "    filtered_tokens = [token for token in parsed_tokens if token in time_dict or token in location_dict or token in vc_dict]    # Compare the noun phrases with dictionaries to find matches terms\n",
    "    filtered_tokens = sorted(filtered_tokens, key=lambda x: len(x), reverse=True)\n",
    "    token_counter = Counter(filtered_tokens)\n",
    "    minus_counter = {}\n",
    "    for token in token_counter.keys():\n",
    "        minus_counter[token] = 0;\n",
    "    tagged_tokens = []\n",
    "    for token, cnt in token_counter.items():\n",
    "        cnt += minus_counter[token]\n",
    "        if cnt == 0: continue\n",
    "        word_tokens = nltk.word_tokenize(token)\n",
    "        if len(word_tokens) > 1:\n",
    "            for wtoken in word_tokens: # Reduce the number of single nouns which are included in a matched noun noun_phrases\n",
    "                try:\n",
    "                    minus_counter[wtoken] -= 1\n",
    "                except: continue\n",
    "        if token in time_dict:\n",
    "            tagged_tokens.append((token, 'TIME', cnt))\n",
    "        if token in location_dict:\n",
    "            tagged_tokens.append((token, 'LOCATION', cnt))\n",
    "        if token in vc_dict:\n",
    "            tagged_tokens.append((token, 'CONCEPT', cnt))\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokens(noun_phrases):\n",
    "    processed_words = []\n",
    "    porter_stem = PorterStemmer()\n",
    "    for word in noun_phrases:\n",
    "        tokens = nltk.word_tokenize(word.lower())\n",
    "        refined_tokenize_list = [unicodedata.normalize('NFKD', token).encode('ascii', 'ignore').decode('utf-8', 'ignore') for token in tokens]\n",
    "        refined_tokenize_list = [word for word in refined_tokenize_list if word not in stop_words]\n",
    "        # refined_tokenize_list = [porter_stem.stem(word) for word in refined_tokenize_list]\n",
    "        # refined_tokenize_list = [word for word in refined_tokenize_list if len(word) > 1 and word.isalpha()]\n",
    "        refined_tokenize_list = [word for word in refined_tokenize_list if len(word) > 1]\n",
    "        complete_word = ' '.join(refined_tokenize_list)\n",
    "        processed_words.append(complete_word)\n",
    "    return processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text_query, number_to_text = False):\n",
    "    text_query = contractions.fix(text_query)\n",
    "    if text_query[-1] != '.': # Append character . at the end of the text query to parse the noun phrases properly\n",
    "        text_query += '.'\n",
    "    tokens = nltk.word_tokenize(text_query)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    if number_to_text is True:\n",
    "        p = inflect.engine()\n",
    "        pos_tags = [(p.number_to_words(item[0]), 'JJ') if item[1] == 'CD' else item for item in pos_tags]\n",
    "    noun_phrases = extract_noun_phrases(pos_tags)\n",
    "    noun_phrases = preprocess_tokens(noun_phrases)\n",
    "    return noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_parse(time_phrase, filters):\n",
    "    su = SUTime(mark_time_ranges = True, include_range = True)\n",
    "    parsed_values = su.parse(time_phrase)\n",
    "    results = []\n",
    "    for value in parsed_values:\n",
    "        print(value)\n",
    "        if value['text'] in filters: continue\n",
    "        if value['type'] == 'DURATION':\n",
    "            begin = value['value']['begin'][1:]\n",
    "            end = value['value']['end'][1:].split('T')[-1]\n",
    "            results.append(f'{begin}->{end}')\n",
    "        elif value['type'] == 'TIME':\n",
    "            temp = value['value'].split('T')\n",
    "            _date = temp[0]\n",
    "            _time = temp[1]\n",
    "            year = _date[:4]\n",
    "            results += [_date, year]\n",
    "            if _time != 'MO': results.append(_time)\n",
    "        elif value['type'] == 'DATE':\n",
    "            temp = value['value'].split(' INTERSECT ')\n",
    "            _date = temp[0]\n",
    "            year = _date[:4]\n",
    "            _time = temp[1]\n",
    "            begin, end, _ = _time.split(',')\n",
    "            results += [_date, year, f'{begin[2:]}-->{end[1:]}']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_this\n",
    "def process_text_query(text_query):\n",
    "    print(text_query)\n",
    "    concepts, locations, _time = text_query.split(';')\n",
    "    if len(concepts) > 0:\n",
    "        print(\"Parse visual concepts\")\n",
    "        parsed_concepts = parse(concepts, number_to_text = True)\n",
    "        print(parsed_concepts)\n",
    "        print('---------------------')\n",
    "    if len(locations) > 0:\n",
    "        print(\"Parse locations\")\n",
    "        parsed_locations = parse(locations)\n",
    "        print(parsed_locations)\n",
    "        print('---------------------')\n",
    "    if len(_time) > 0:\n",
    "        print(\"Parse date and time\")\n",
    "        parsed_time = parse(_time)\n",
    "        parsed_time += time_parse(_time, parsed_time)\n",
    "        print(parsed_time)\n",
    "        print('---------------------')\n",
    "    # tagged_tokens = analyse(parsed_text)\n",
    "    # print(tagged_tokens)\n",
    "    # result = match(tagged_tokens)\n",
    "    # if len(result) == 0:\n",
    "    #     result = match(tagged_tokens, degree='normal')\n",
    "    # return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try text-parsing feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hot dog, fire house, pizza; park; May 2018 at 14:00\n",
      "Parse visual concepts\n",
      "['hot dog', 'dog', 'fire house', 'house', 'pizza', 'fire']\n",
      "---------------------\n",
      "Parse locations\n",
      "['park']\n",
      "---------------------\n",
      "Parse date and time\n",
      "{'timex-value': '2018-05T14:00', 'start': 1, 'end': 18, 'text': 'May 2018 at 14:00', 'type': 'TIME', 'value': '2018-05T14:00'}\n",
      "['2018 may', 'may', '2018-05', '2018', '14:00']\n",
      "---------------------\n",
      "Function process_text_query elapsed time: 0:00:00.037316\n"
     ]
    }
   ],
   "source": [
    "# query = '4 people with a dog; park; in early morning from 8am to 10am on 2021-02-25'\n",
    "# query = '4 people with a dog; park; in early morning on 2021-02-25 from 8:00 to 10:30'\n",
    "# query = ';;from 2pm to 5pm on November 2021'\n",
    "# query = 'hot dog, fire house, pizza; park; May 2018 at 14:00'\n",
    "query = ';;from 2pm to 5pm on 2021-02-25'\n",
    "process_text_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
